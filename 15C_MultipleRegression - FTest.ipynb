{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c47bef66-afec-437a-a3ba-652122594f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:\n",
      "         y  x1    x2\n",
      "0   0.2454   1  60.0\n",
      "1   0.2569   1  60.0\n",
      "2   0.4984   1  60.0\n",
      "3   0.2531   2  60.0\n",
      "4   0.3143   2  60.0\n",
      "5   0.5316   2  60.0\n",
      "6   0.5612   3  60.0\n",
      "7   1.0346   3  60.0\n",
      "8   1.3985   3  60.0\n",
      "9   0.2466   1  72.5\n",
      "10  0.2581   1  72.5\n",
      "11  0.2566   1  72.5\n",
      "12  0.2437   2  72.5\n",
      "13  0.2703   2  72.5\n",
      "14  0.4845   2  72.5\n",
      "15  0.3027   3  72.5\n",
      "16  0.8672   3  72.5\n",
      "17  1.2242   3  72.5\n",
      "18  0.2519   1  85.0\n",
      "19  0.2546   1  85.0\n",
      "20  0.2496   1  85.0\n",
      "21  0.2545   2  85.0\n",
      "22  0.2562   2  85.0\n",
      "23  0.2904   2  85.0\n",
      "24  0.2504   3  85.0\n",
      "25  0.2528   3  85.0\n",
      "26  0.3007   3  85.0\n",
      "\n",
      "Regression Equation:\n",
      "y = 0.9023 + (0.2041)*x1 + (-0.0121)*x2\n",
      "\n",
      "F-statistic: 9.1367\n",
      "F-test p-value: 0.001121\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter significance level (default 0.05):  .05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Variables significant at alpha = 0.05:\n",
      "const: p-value = 0.021971 → Significant\n",
      "x1: p-value = 0.002190 → Significant\n",
      "x2: p-value = 0.017528 → Significant\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------\n",
    "# Use for Regression Equation, FStat, Pvalue\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# === Step 1: Load dataset ===\n",
    "file_path = r\"C:\\Users\\300393449\\OneDrive - Douglas College\\Documents\\4th Semester\\2_Business_Statistics_II\\Python\\DataScratch.xlsx\"\n",
    "sheet_name = \"Reg_Model\"\n",
    "\n",
    "df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "print(\"Dataset:\")\n",
    "print(df)\n",
    "\n",
    "# === Helpers ===\n",
    "def clean_numeric_series(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Strip $, %, commas, spaces; coerce to numeric (NaN on failure).\"\"\"\n",
    "    s = s.astype(str).str.replace(r'[\\$,%]', '', regex=True).str.replace(',', '', regex=False).str.strip()\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def coerce_all_numeric(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = X.copy()\n",
    "    for c in out.columns:\n",
    "        if out[c].dtype == 'object' or str(out[c].dtype).startswith('category'):\n",
    "            out[c] = clean_numeric_series(out[c])\n",
    "        else:\n",
    "            # try safe numeric cast; if fails, becomes NaN\n",
    "            out[c] = pd.to_numeric(out[c], errors='coerce')\n",
    "    return out\n",
    "\n",
    "def handle_categoricals(data: pd.DataFrame, custom_orders: dict | None = None):\n",
    "    \"\"\"Excel-style dummy encoding for any categorical predictors (exclude 'y').\"\"\"\n",
    "    d = data.copy()\n",
    "    custom_orders = custom_orders or {}\n",
    "    cat_cols = [c for c in d.columns if c != 'y' and (d[c].dtype == 'object' or str(d[c].dtype).startswith('category'))]\n",
    "\n",
    "    baseline_info = {}\n",
    "    if not cat_cols:\n",
    "        return d, baseline_info\n",
    "\n",
    "    # enforce custom baselines if provided; else baseline = first alphabetical\n",
    "    for col in cat_cols:\n",
    "        if col in custom_orders:\n",
    "            d[col] = pd.Categorical(d[col], categories=custom_orders[col], ordered=True)\n",
    "            baseline_info[col] = custom_orders[col][0]\n",
    "        else:\n",
    "            vals = d[col].astype(str).str.strip()\n",
    "            cats_sorted = sorted(vals.unique().tolist())\n",
    "            d[col] = pd.Categorical(vals, categories=cats_sorted, ordered=True)\n",
    "            baseline_info[col] = cats_sorted[0] if cats_sorted else None\n",
    "\n",
    "    d = pd.get_dummies(d, columns=cat_cols, drop_first=True)\n",
    "    return d, baseline_info\n",
    "\n",
    "# === Step 2: Dummy-encode any categoricals (Excel-style) ===\n",
    "custom_baselines = {}  # e.g., {'Month': ['June','July','August']}\n",
    "df_dum, baseline_info = handle_categoricals(df, custom_baselines)\n",
    "\n",
    "if baseline_info:\n",
    "    print(\"\\n[Dummy encoding summary] (baseline absorbed by intercept):\")\n",
    "    for col, base in baseline_info.items():\n",
    "        created = [c for c in df_dum.columns if c.startswith(f\"{col}_\")]\n",
    "        print(f\" - {col}: baseline = '{base}', dummies = {created}\")\n",
    "\n",
    "# === Step 3: Build model matrices ===\n",
    "if 'y' not in df_dum.columns:\n",
    "    raise ValueError(\"Dataset must contain a 'y' column.\")\n",
    "\n",
    "y = df_dum['y']\n",
    "X = df_dum.drop(columns=['y'])\n",
    "\n",
    "# --- DIAGNOSTIC: show offending dtypes before coercion ---\n",
    "non_numeric_cols = [c for c in X.columns if (X[c].dtype == 'object' or str(X[c].dtype).startswith('category'))]\n",
    "if y.dtype == 'object' or str(y.dtype).startswith('category') or non_numeric_cols:\n",
    "    print(\"\\n[Info] Coercing non-numeric columns to numeric...\")\n",
    "    if y.dtype == 'object' or str(y.dtype).startswith('category'):\n",
    "        print(\" - y is non-numeric; cleaning it.\")\n",
    "    if non_numeric_cols:\n",
    "        print(\" - Non-numeric predictors:\", non_numeric_cols)\n",
    "\n",
    "# Coerce to numeric safely (currency, commas, percents -> handled)\n",
    "y = clean_numeric_series(y)\n",
    "X = coerce_all_numeric(X)\n",
    "\n",
    "# Drop rows with missing y or any missing X\n",
    "mask = y.notna() & X.notna().all(axis=1)\n",
    "dropped_rows = int(len(y) - mask.sum())\n",
    "if dropped_rows > 0:\n",
    "    print(f\"\\n[Warning] Dropping {dropped_rows} row(s) with non-numeric or missing values after cleaning.\")\n",
    "y = y[mask]\n",
    "X = X[mask]\n",
    "\n",
    "# Drop predictors that are constant or entirely NaN\n",
    "const_or_empty = [c for c in X.columns if X[c].nunique(dropna=True) <= 1]\n",
    "if const_or_empty:\n",
    "    print(f\"[Info] Dropping constant/empty column(s): {const_or_empty}\")\n",
    "    X = X.drop(columns=const_or_empty)\n",
    "\n",
    "# Final safety: ensure float dtype\n",
    "X = X.astype(float)\n",
    "y = y.astype(float)\n",
    "\n",
    "# Add intercept\n",
    "X = sm.add_constant(X, has_constant='add')\n",
    "\n",
    "# === Step 4: Fit OLS ===\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# === Step 5: Regression equation ===\n",
    "coef = model.params\n",
    "eq_parts = [f\"y = {coef['const']:.4f}\"]\n",
    "for col in X.columns:\n",
    "    if col == 'const':\n",
    "        continue\n",
    "    eq_parts.append(f\"+ ({coef[col]:.4f})*{col}\")\n",
    "equation = \" \".join(eq_parts)\n",
    "\n",
    "print(\"\\nRegression Equation:\")\n",
    "print(equation)\n",
    "\n",
    "# === Step 6: Overall F-test ===\n",
    "print(f\"\\nF-statistic: {model.fvalue:.4f}\")\n",
    "print(f\"F-test p-value: {model.f_pvalue:.6f}\")\n",
    "\n",
    "# === Step 7: Significance test per variable ===\n",
    "alpha_input = input(\"\\nEnter significance level (default 0.05): \").strip()\n",
    "alpha = float(alpha_input) if alpha_input else 0.05\n",
    "\n",
    "print(f\"\\nVariables significant at alpha = {alpha}:\")\n",
    "for var, pval in model.pvalues.items():\n",
    "    status = \"Significant\" if pval < alpha else \"Not significant\"\n",
    "    print(f\"{var}: p-value = {pval:.6f} → {status}\")\n",
    "\n",
    "# === Step 8: Interpret dummies relative to baseline ===\n",
    "if baseline_info:\n",
    "    print(\"\\nInterpretation of dummy variables (relative to baseline):\")\n",
    "    for cat_col, base in baseline_info.items():\n",
    "        for dcol in [c for c in X.columns if c.startswith(f\"{cat_col}_\")]:\n",
    "            level = dcol.split(f\"{cat_col}_\", 1)[1]\n",
    "            print(f\"- {dcol}: Expected change in y when {cat_col} = '{level}' vs '{base}', holding other predictors constant.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a627d0e9-533e-43e6-8962-be8f3675ac69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:\n",
      "         y  x1    x2\n",
      "0   0.2454   1  60.0\n",
      "1   0.2569   1  60.0\n",
      "2   0.4984   1  60.0\n",
      "3   0.2531   2  60.0\n",
      "4   0.3143   2  60.0\n",
      "5   0.5316   2  60.0\n",
      "6   0.5612   3  60.0\n",
      "7   1.0346   3  60.0\n",
      "8   1.3985   3  60.0\n",
      "9   0.2466   1  72.5\n",
      "10  0.2581   1  72.5\n",
      "11  0.2566   1  72.5\n",
      "12  0.2437   2  72.5\n",
      "13  0.2703   2  72.5\n",
      "14  0.4845   2  72.5\n",
      "15  0.3027   3  72.5\n",
      "16  0.8672   3  72.5\n",
      "17  1.2242   3  72.5\n",
      "18  0.2519   1  85.0\n",
      "19  0.2546   1  85.0\n",
      "20  0.2496   1  85.0\n",
      "21  0.2545   2  85.0\n",
      "22  0.2562   2  85.0\n",
      "23  0.2904   2  85.0\n",
      "24  0.2504   3  85.0\n",
      "25  0.2528   3  85.0\n",
      "26  0.3007   3  85.0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Use Excel-style manual dummy setup? (y/N):  N\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Skipping manual dummy setup. (You can still rely on automatic cleaning and fitting.)\n",
      "\n",
      "Regression Equation:\n",
      "y = 0.9023 + (0.2041)*x1 + (-0.0121)*x2\n",
      "\n",
      "F-statistic: 9.1367\n",
      "F-test p-value: 0.001121\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter significance level (default 0.05):  .05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Variables significant at alpha = 0.05:\n",
      "const: p-value = 0.021971 → Significant\n",
      "x1: p-value = 0.002190 → Significant\n",
      "x2: p-value = 0.017528 → Significant\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# === Step 1: Load dataset ===\n",
    "file_path = r\"C:\\Users\\300393449\\OneDrive - Douglas College\\Documents\\4th Semester\\2_Business_Statistics_II\\Python\\DataScratch.xlsx\"\n",
    "sheet_name = \"Reg_Model\"\n",
    "\n",
    "df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "print(\"Dataset:\")\n",
    "print(df)\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def clean_numeric_series(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Strip $, %, commas, spaces; coerce to numeric (NaN on failure).\"\"\"\n",
    "    s = s.astype(str).str.replace(r'[\\$,%]', '', regex=True).str.replace(',', '', regex=False).str.strip()\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def coerce_all_numeric(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = X.copy()\n",
    "    for c in out.columns:\n",
    "        if out[c].dtype == 'object' or str(out[c].dtype).startswith('category'):\n",
    "            out[c] = clean_numeric_series(out[c])\n",
    "        else:\n",
    "            out[c] = pd.to_numeric(out[c], errors='coerce')\n",
    "    return out\n",
    "\n",
    "def manual_excel_dummy(df_in: pd.DataFrame, cat_col: str, dummy_prefix: str | None = None,\n",
    "                       categories_order: list[str] | None = None) -> tuple[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Create Excel-style dummies for cat_col:\n",
    "      - If there are C categories, create C-1 dummy cols.\n",
    "      - The *first* category in categories_order is the baseline (all zeros).\n",
    "      - Original string column is dropped.\n",
    "      - Returns (df_out, dummy_info) where dummy_info describes mapping & baseline.\n",
    "\n",
    "    dummy_info structure:\n",
    "      {\n",
    "        'column': cat_col,\n",
    "        'baseline': <baseline category>,\n",
    "        'dummies': ['<prefix>_D1', '<prefix>_D2', ...],\n",
    "        'mapping': { category: [d1,d2,...] }\n",
    "      }\n",
    "    \"\"\"\n",
    "    d = df_in.copy()\n",
    "\n",
    "    if cat_col not in d.columns:\n",
    "        raise ValueError(f\"Column '{cat_col}' not found in dataframe.\")\n",
    "\n",
    "    # Clean values to strings for stable matching\n",
    "    cats = d[cat_col].astype(str).str.strip()\n",
    "    uniq = cats.unique().tolist()\n",
    "\n",
    "    # If user didn't supply explicit order, default to alphabetical\n",
    "    if categories_order:\n",
    "        order = [c.strip() for c in categories_order]\n",
    "        missing = [c for c in uniq if c not in order]\n",
    "        if missing:\n",
    "            raise ValueError(f\"These categories exist in data but not in your order: {missing}\")\n",
    "    else:\n",
    "        order = sorted(uniq)\n",
    "\n",
    "    if len(order) < 2:\n",
    "        raise ValueError(f\"Need at least 2 categories in '{cat_col}' to create dummies; found {len(order)}.\")\n",
    "\n",
    "    baseline = order[0]                 # Excel-style: first is baseline\n",
    "    k = len(order) - 1                  # number of dummy columns\n",
    "    if dummy_prefix is None:\n",
    "        dummy_prefix = f\"{cat_col}\"\n",
    "\n",
    "    dummy_cols = [f\"{dummy_prefix}_D{i+1}\" for i in range(k)]\n",
    "\n",
    "    # Build mapping: category -> dummy vector (length k)\n",
    "    # baseline (first) is all zeros; category i (1..k) has 1 in position i, zeros elsewhere\n",
    "    mapping = {}\n",
    "    for idx, cat in enumerate(order):\n",
    "        vec = [0]*k\n",
    "        if idx > 0:\n",
    "            vec[idx-1] = 1\n",
    "        mapping[cat] = vec\n",
    "\n",
    "    # Create the actual dummy columns\n",
    "    for j in range(k):\n",
    "        d[dummy_cols[j]] = 0  # initialize\n",
    "\n",
    "    # Apply mapping row-wise\n",
    "    for i in range(len(d)):\n",
    "        cat_val = str(d.loc[i, cat_col]).strip()\n",
    "        if cat_val not in mapping:\n",
    "            # unseen category → treat like baseline (all zeros)\n",
    "            vec = [0]*k\n",
    "        else:\n",
    "            vec = mapping[cat_val]\n",
    "        for j in range(k):\n",
    "            d.at[i, dummy_cols[j]] = vec[j]\n",
    "\n",
    "    # Drop original categorical column\n",
    "    d = d.drop(columns=[cat_col])\n",
    "\n",
    "    # Prepare info for interpretation / display\n",
    "    info = {\n",
    "        'column': cat_col,\n",
    "        'baseline': baseline,\n",
    "        'dummies': dummy_cols,\n",
    "        'mapping': mapping\n",
    "    }\n",
    "    return d, info\n",
    "\n",
    "# =========================\n",
    "# Step 2: Optional MANUAL (Excel-style) dummy definition\n",
    "# =========================\n",
    "use_manual = input(\"\\nUse Excel-style manual dummy setup? (y/N): \").strip().lower() == 'y'\n",
    "dummy_meta_list = []  # store info for all manually encoded columns\n",
    "\n",
    "if use_manual:\n",
    "    print(\"\\nAvailable columns:\")\n",
    "    for c in df.columns:\n",
    "        print(\" -\", c)\n",
    "\n",
    "    while True:\n",
    "        col_name = input(\"\\nEnter the NAME of the string/categorical column to dummy-encode (or press Enter to stop): \").strip()\n",
    "        if not col_name:\n",
    "            break\n",
    "\n",
    "        # Show distinct categories for convenience\n",
    "        if col_name in df.columns:\n",
    "            unique_vals = sorted(df[col_name].astype(str).str.strip().unique().tolist())\n",
    "            print(f\"Distinct values in '{col_name}': {unique_vals}\")\n",
    "        else:\n",
    "            print(f\"[Warning] '{col_name}' not found. Try again.\")\n",
    "            continue\n",
    "\n",
    "        # Ask user for category order (baseline = first)\n",
    "        custom_order_in = input(\n",
    "            \"Enter category order separated by commas (first = baseline). \"\n",
    "            \"Press Enter to use alphabetical order: \"\n",
    "        ).strip()\n",
    "\n",
    "        if custom_order_in:\n",
    "            custom_order = [x.strip() for x in custom_order_in.split(',') if x.strip() != \"\"]\n",
    "        else:\n",
    "            custom_order = None  # fallback to alphabetical\n",
    "\n",
    "        # Prefix for dummy columns\n",
    "        prefix_in = input(\"Enter a prefix for the dummy columns (press Enter to use column name): \").strip()\n",
    "        prefix = prefix_in if prefix_in else col_name\n",
    "\n",
    "        # Build manual dummies\n",
    "        df, dmeta = manual_excel_dummy(df, col_name, dummy_prefix=prefix, categories_order=custom_order)\n",
    "        dummy_meta_list.append(dmeta)\n",
    "\n",
    "        # Show mapping like Excel\n",
    "        print(\"\\n[Manual dummy mapping]\")\n",
    "        print(f\"Column: {dmeta['column']}, baseline = '{dmeta['baseline']}', dummy columns = {dmeta['dummies']}\")\n",
    "        for cat, vec in dmeta['mapping'].items():\n",
    "            print(f\"  {cat}: {vec}\")\n",
    "\n",
    "        # Continue encoding more categorical columns?\n",
    "        more = input(\"\\nAdd another column? (y/N): \").strip().lower()\n",
    "        if more != 'y':\n",
    "            break\n",
    "else:\n",
    "    print(\"\\nSkipping manual dummy setup. (You can still rely on automatic cleaning and fitting.)\")\n",
    "\n",
    "# =========================\n",
    "# Step 3: Build model matrices (y and X)\n",
    "# =========================\n",
    "if 'y' not in df.columns:\n",
    "    raise ValueError(\"Dataset must contain a 'y' column (dependent variable).\")\n",
    "\n",
    "y = df['y']\n",
    "X = df.drop(columns=['y'])\n",
    "\n",
    "# =========================\n",
    "# Step 4: Clean to numeric\n",
    "# =========================\n",
    "# Show any non-numeric columns before coercion (for info)\n",
    "pre_non_num = [c for c in X.columns if (X[c].dtype == 'object' or str(X[c].dtype).startswith('category'))]\n",
    "if y.dtype == 'object' or str(y.dtype).startswith('category') or pre_non_num:\n",
    "    print(\"\\n[Info] Coercing non-numeric columns to numeric where possible (stripping $, %, commas).\")\n",
    "    if y.dtype == 'object' or str(y.dtype).startswith('category'):\n",
    "        print(\" - y is non-numeric; cleaning it.\")\n",
    "    if pre_non_num:\n",
    "        print(\" - Non-numeric predictors:\", pre_non_num)\n",
    "\n",
    "y = clean_numeric_series(y)\n",
    "X = coerce_all_numeric(X)\n",
    "\n",
    "# Drop rows with missing y or any missing X\n",
    "mask = y.notna() & X.notna().all(axis=1)\n",
    "dropped_rows = int(len(y) - mask.sum())\n",
    "if dropped_rows > 0:\n",
    "    print(f\"\\n[Warning] Dropping {dropped_rows} row(s) with non-numeric or missing values after cleaning.\")\n",
    "y = y[mask]\n",
    "X = X[mask]\n",
    "\n",
    "# Drop predictors that are constant or entirely NaN\n",
    "const_or_empty = [c for c in X.columns if X[c].nunique(dropna=True) <= 1]\n",
    "if const_or_empty:\n",
    "    print(f\"[Info] Dropping constant/empty column(s): {const_or_empty}\")\n",
    "    X = X.drop(columns=const_or_empty)\n",
    "\n",
    "# Final numeric dtype\n",
    "X = X.astype(float)\n",
    "y = y.astype(float)\n",
    "\n",
    "# Add intercept\n",
    "X = sm.add_constant(X, has_constant='add')\n",
    "\n",
    "# =========================\n",
    "# Step 5: Fit OLS\n",
    "# =========================\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# =========================\n",
    "# Step 6: Regression equation (copy-ready)\n",
    "# =========================\n",
    "coef = model.params\n",
    "eq_parts = [f\"y = {coef['const']:.4f}\"]\n",
    "for col in X.columns:\n",
    "    if col == 'const':\n",
    "        continue\n",
    "    eq_parts.append(f\"+ ({coef[col]:.4f})*{col}\")\n",
    "equation = \" \".join(eq_parts)\n",
    "\n",
    "print(\"\\nRegression Equation:\")\n",
    "print(equation)\n",
    "\n",
    "# =========================\n",
    "# Step 7: Overall F-test\n",
    "# =========================\n",
    "print(f\"\\nF-statistic: {model.fvalue:.4f}\")\n",
    "print(f\"F-test p-value: {model.f_pvalue:.6f}\")\n",
    "\n",
    "# =========================\n",
    "# Step 8: Per-variable significance\n",
    "# =========================\n",
    "alpha_input = input(\"\\nEnter significance level (default 0.05): \").strip()\n",
    "alpha = float(alpha_input) if alpha_input else 0.05\n",
    "\n",
    "print(f\"\\nVariables significant at alpha = {alpha}:\")\n",
    "for var, pval in model.pvalues.items():\n",
    "    status = \"Significant\" if pval < alpha else \"Not significant\"\n",
    "    print(f\"{var}: p-value = {pval:.6f} → {status}\")\n",
    "\n",
    "# =========================\n",
    "# Step 9: Interpret dummy variables (manual Excel-style mapping)\n",
    "# =========================\n",
    "if dummy_meta_list:\n",
    "    print(\"\\nInterpretation of manually created dummy variables (relative to baseline):\")\n",
    "    for meta in dummy_meta_list:\n",
    "        base = meta['baseline']\n",
    "        for idx, dcol in enumerate(meta['dummies']):\n",
    "            # The category corresponding to this dummy is the (idx+1)-th category in the order\n",
    "            # Recover the level by finding which category has vector with 1 at idx\n",
    "            target_level = None\n",
    "            for cat, vec in meta['mapping'].items():\n",
    "                if idx < len(vec) and vec[idx] == 1:\n",
    "                    target_level = cat\n",
    "                    break\n",
    "            if target_level is None:\n",
    "                target_level = f\"Level_{idx+1}\"\n",
    "            print(f\"- {dcol}: Expected change in y when {meta['column']} = '{target_level}' vs '{base}', holding other predictors constant.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "637121b7-6044-459b-a508-2da939a513ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:\n",
      "         y  x1    x2\n",
      "0   0.2454   1  60.0\n",
      "1   0.2569   1  60.0\n",
      "2   0.4984   1  60.0\n",
      "3   0.2531   2  60.0\n",
      "4   0.3143   2  60.0\n",
      "5   0.5316   2  60.0\n",
      "6   0.5612   3  60.0\n",
      "7   1.0346   3  60.0\n",
      "8   1.3985   3  60.0\n",
      "9   0.2466   1  72.5\n",
      "10  0.2581   1  72.5\n",
      "11  0.2566   1  72.5\n",
      "12  0.2437   2  72.5\n",
      "13  0.2703   2  72.5\n",
      "14  0.4845   2  72.5\n",
      "15  0.3027   3  72.5\n",
      "16  0.8672   3  72.5\n",
      "17  1.2242   3  72.5\n",
      "18  0.2519   1  85.0\n",
      "19  0.2546   1  85.0\n",
      "20  0.2496   1  85.0\n",
      "21  0.2545   2  85.0\n",
      "22  0.2562   2  85.0\n",
      "23  0.2904   2  85.0\n",
      "24  0.2504   3  85.0\n",
      "25  0.2528   3  85.0\n",
      "26  0.3007   3  85.0\n",
      "\n",
      "Regression Equation:\n",
      "y = 0.9023 + (0.2041)*x1 + (-0.0121)*x2\n",
      "\n",
      "F-statistic: 9.1367\n",
      "F-test p-value: 0.001121\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter significance level (default 0.05):  .05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Variables significant at alpha = 0.05:\n",
      "const: p-value = 0.021971 → Significant\n",
      "x1: p-value = 0.002190 → Significant\n",
      "x2: p-value = 0.017528 → Significant\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# === Step 1: Load dataset ===\n",
    "file_path = r\"C:\\Users\\300393449\\OneDrive - Douglas College\\Documents\\4th Semester\\2_Business_Statistics_II\\Python\\DataScratch.xlsx\"\n",
    "sheet_name = \"Reg_Model\"\n",
    "\n",
    "df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "print(\"Dataset:\")\n",
    "print(df)\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def clean_numeric_series(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Strip $, %, commas, spaces; coerce to numeric (NaN on failure).\"\"\"\n",
    "    s = s.astype(str).str.replace(r'[\\$,%]', '', regex=True).str.replace(',', '', regex=False).str.strip()\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "def coerce_all_numeric(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = X.copy()\n",
    "    for c in out.columns:\n",
    "        if out[c].dtype == 'object' or str(out[c].dtype).startswith('category'):\n",
    "            out[c] = clean_numeric_series(out[c])\n",
    "        else:\n",
    "            out[c] = pd.to_numeric(out[c], errors='coerce')\n",
    "    return out\n",
    "\n",
    "def build_c_minus_1_dummies(df_in: pd.DataFrame, exclude: set[str]) -> tuple[pd.DataFrame, list[dict]]:\n",
    "    \"\"\"\n",
    "    For every object/category column not in `exclude`,\n",
    "    create (C-1) dummy columns (Excel-style) using an alphabetical baseline.\n",
    "    Drop original categorical column.\n",
    "    Return (df_out, meta_list) where meta_list holds mapping/baseline per column.\n",
    "    \"\"\"\n",
    "    d = df_in.copy()\n",
    "    meta_list = []\n",
    "\n",
    "    # Identify categorical columns (excluding provided columns)\n",
    "    cat_cols = [c for c in d.columns\n",
    "                if c not in exclude and (d[c].dtype == 'object' or str(d[c].dtype).startswith('category'))]\n",
    "\n",
    "    for col in cat_cols:\n",
    "        # Clean strings and get unique ordered categories\n",
    "        col_vals = d[col].astype(str).str.strip()\n",
    "        uniques = sorted(pd.Index(col_vals.unique()).tolist())\n",
    "        C = len(uniques)\n",
    "\n",
    "        if C < 2:\n",
    "            # Nothing to dummy-encode if only one unique value; just drop it (constant)\n",
    "            d = d.drop(columns=[col])\n",
    "            continue\n",
    "\n",
    "        baseline = uniques[0]               # Excel-style baseline = first alphabetically\n",
    "        k = C - 1                           # number of dummy columns\n",
    "        dummy_cols = [f\"{col}_D{i+1}\" for i in range(k)]\n",
    "\n",
    "        # Category -> vector mapping (baseline = all zeros; category i has 1 at position i)\n",
    "        mapping = {}\n",
    "        for idx, cat in enumerate(uniques):\n",
    "            vec = [0]*k\n",
    "            if idx > 0:\n",
    "                vec[idx-1] = 1\n",
    "            mapping[cat] = vec\n",
    "\n",
    "        # Create actual dummy columns\n",
    "        for j in range(k):\n",
    "            d[dummy_cols[j]] = 0\n",
    "\n",
    "        # Assign vectors row-wise\n",
    "        for i in range(len(d)):\n",
    "            cat_val = str(d.loc[i, col]).strip()\n",
    "            vec = mapping.get(cat_val, [0]*k)  # unseen -> baseline\n",
    "            for j in range(k):\n",
    "                d.at[i, dummy_cols[j]] = vec[j]\n",
    "\n",
    "        # Drop original categorical column\n",
    "        d = d.drop(columns=[col])\n",
    "\n",
    "        # Save metadata for interpretation\n",
    "        meta_list.append({\n",
    "            'column': col,\n",
    "            'baseline': baseline,\n",
    "            'dummies': dummy_cols,\n",
    "            'mapping': mapping\n",
    "        })\n",
    "\n",
    "        # Show summary for this column\n",
    "        print(f\"\\n[Dummy encoding] '{col}': C={C} → {k} columns, baseline = '{baseline}'\")\n",
    "        for cat in uniques:\n",
    "            print(f\"  {cat}: {mapping[cat]}\")\n",
    "\n",
    "    return d, meta_list\n",
    "\n",
    "# =========================\n",
    "# Step 2: Auto (C-1) dummy-encode ALL categorical columns (except 'y')\n",
    "# =========================\n",
    "df_enc, dummy_meta_list = build_c_minus_1_dummies(df, exclude={'y'})\n",
    "\n",
    "# =========================\n",
    "# Step 3: Build model matrices (y, X)\n",
    "# =========================\n",
    "if 'y' not in df_enc.columns:\n",
    "    raise ValueError(\"Dataset must contain a 'y' column (dependent variable).\")\n",
    "\n",
    "y = df_enc['y']\n",
    "X = df_enc.drop(columns=['y'])\n",
    "\n",
    "# =========================\n",
    "# Step 4: Clean to numeric, drop bad rows/columns\n",
    "# =========================\n",
    "# Inform about non-numeric before coercion\n",
    "pre_non_num = [c for c in X.columns if (X[c].dtype == 'object' or str(X[c].dtype).startswith('category'))]\n",
    "if y.dtype == 'object' or str(y.dtype).startswith('category') or pre_non_num:\n",
    "    print(\"\\n[Info] Coercing non-numeric columns to numeric (stripping $, %, commas).\")\n",
    "    if y.dtype == 'object' or str(y.dtype).startswith('category'):\n",
    "        print(\" - y is non-numeric; cleaning it.\")\n",
    "    if pre_non_num:\n",
    "        print(\" - Non-numeric predictors:\", pre_non_num)\n",
    "\n",
    "y = clean_numeric_series(y)\n",
    "X = coerce_all_numeric(X)\n",
    "\n",
    "# Drop rows with missing y or any missing X\n",
    "mask = y.notna() & X.notna().all(axis=1)\n",
    "dropped_rows = int(len(y) - mask.sum())\n",
    "if dropped_rows > 0:\n",
    "    print(f\"\\n[Warning] Dropping {dropped_rows} row(s) with non-numeric or missing values after cleaning.\")\n",
    "y = y[mask]\n",
    "X = X[mask]\n",
    "\n",
    "# Drop predictors that are constant/empty\n",
    "const_or_empty = [c for c in X.columns if X[c].nunique(dropna=True) <= 1]\n",
    "if const_or_empty:\n",
    "    print(f\"[Info] Dropping constant/empty column(s): {const_or_empty}\")\n",
    "    X = X.drop(columns=const_or_empty)\n",
    "\n",
    "# Final numeric dtype\n",
    "X = X.astype(float)\n",
    "y = y.astype(float)\n",
    "\n",
    "# Add intercept\n",
    "X = sm.add_constant(X, has_constant='add')\n",
    "\n",
    "# =========================\n",
    "# Step 5: Fit OLS\n",
    "# =========================\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# =========================\n",
    "# Step 6: Regression equation (copy-ready)\n",
    "# =========================\n",
    "coef = model.params\n",
    "eq_parts = [f\"y = {coef['const']:.4f}\"]\n",
    "for col in X.columns:\n",
    "    if col == 'const':\n",
    "        continue\n",
    "    eq_parts.append(f\"+ ({coef[col]:.4f})*{col}\")\n",
    "equation = \" \".join(eq_parts)\n",
    "\n",
    "print(\"\\nRegression Equation:\")\n",
    "print(equation)\n",
    "\n",
    "# =========================\n",
    "# Step 7: Overall F-test\n",
    "# =========================\n",
    "print(f\"\\nF-statistic: {model.fvalue:.4f}\")\n",
    "print(f\"F-test p-value: {model.f_pvalue:.6f}\")\n",
    "\n",
    "# =========================\n",
    "# Step 8: Per-variable significance\n",
    "# =========================\n",
    "alpha_input = input(\"\\nEnter significance level (default 0.05): \").strip()\n",
    "alpha = float(alpha_input) if alpha_input else 0.05\n",
    "\n",
    "print(f\"\\nVariables significant at alpha = {alpha}:\")\n",
    "for var, pval in model.pvalues.items():\n",
    "    status = \"Significant\" if pval < alpha else \"Not significant\"\n",
    "    print(f\"{var}: p-value = {pval:.6f} → {status}\")\n",
    "\n",
    "# =========================\n",
    "# Step 9: Interpret dummy variables (relative to baseline)\n",
    "# =========================\n",
    "if dummy_meta_list:\n",
    "    print(\"\\nInterpretation of dummy variables (relative to baseline):\")\n",
    "    for meta in dummy_meta_list:\n",
    "        base = meta['baseline']\n",
    "        for idx, dcol in enumerate(meta['dummies']):\n",
    "            # find which category corresponds to this dummy (vector with 1 at idx)\n",
    "            level = None\n",
    "            for cat, vec in meta['mapping'].items():\n",
    "                if idx < len(vec) and vec[idx] == 1:\n",
    "                    level = cat\n",
    "                    break\n",
    "            if level is None:\n",
    "                level = f\"Level_{idx+1}\"\n",
    "            print(f\"- {dcol}: Expected change in y when {meta['column']} = '{level}' vs '{base}', holding other predictors constant.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8f2b22-e4f8-47a8-9791-efedae5fe302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
